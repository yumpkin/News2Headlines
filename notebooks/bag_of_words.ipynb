{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "advance-ivory",
   "metadata": {},
   "source": [
    "```\n",
    "We need to bu bag of words to be used for topic modelling\n",
    "Accordig to bigARTM a dataset like UCI bag-of-words has a suitable format,\n",
    "so we would make a bag-of-word vectors out of our own dataset in the same format as the UCI one.\n",
    "\n",
    "For each text collection, D is the number of documents, W is the\n",
    "number of words in the vocabulary, and N is the total number of words\n",
    "in the collection (below, NNZ is the number of nonzero counts in the\n",
    "bag-of-words). After tokenization and removal of stopwords, the\n",
    "vocabulary of unique words was truncated by only keeping words that\n",
    "occurred more than ten times.\n",
    "\n",
    "These data sets are ideal\n",
    "for clustering and topic modeling experiments.\n",
    "\n",
    "For each text collection we provide docword.*.txt (the bag of words\n",
    "file in sparse format) and vocab.*.txt (the vocab file).\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "explicit-notebook",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/aliak/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/aliak/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import nltk, razdel\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "# Download nltk packages used in this example\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "broke-secret",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('../dataset/enumerated_shuffled_rbc_dataset.json')\n",
    "df = df.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "creative-rings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize list of stopwords as needed. Here, we append common\n",
    "# punctuation and contraction artifacts.\n",
    "with open('../aux/stopwords-ru.txt', 'r') as f:\n",
    "    ru_stop_words_extensive = f.read().splitlines()\n",
    "    \n",
    "punctuations = list(string.punctuation) + [\"—\", \"«\", \"»\", \"\\n\"]\n",
    "stop_words = list(set(ru_stop_words_extensive + stopwords.words('russian'))) + punctuations\n",
    "\n",
    "def get_article_sentences(article_text):\n",
    "    sentences = list()\n",
    "    for sentence in razdel.sentenize(article_text):\n",
    "        sentences.append(sentence.text)\n",
    "    return sentences\n",
    "\n",
    "def get_article_tokens(article_sentences):\n",
    "    tokens = list()\n",
    "    for sentence in article_sentences:\n",
    "        for token in razdel.tokenize(sentence):\n",
    "            if token.text not in stop_words:\n",
    "                tokens.append(token.text.lower())\n",
    "    return tokens\n",
    "\n",
    "def get_article_lemmas(article_sentences):\n",
    "    mystem = Mystem()\n",
    "    lemmas = list()\n",
    "    for sentence in article_sentences:\n",
    "        sentence_lemmas = mystem.lemmatize(sentence.lower())\n",
    "        sentence_lemmas = [lemma for lemma in sentence_lemmas if lemma not in stop_words\\\n",
    "          and lemma != \" \"\\\n",
    "          and not lemma.isdigit()\n",
    "          and lemma.strip() not in punctuations]\n",
    "    lemmas+=sentence_lemmas\n",
    "    return set(lemmas)\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r\"[^а-яА-Я]\", \" \", doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    sentences = get_article_sentences(doc)\n",
    "    # tokenize document\n",
    "    lemmas = get_article_lemmas(sentences)\n",
    "    #filter stopwords out of document\n",
    "    tokens = get_article_tokens(sentences)\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(lemmas)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "religious-richards",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1606"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = normalize_corpus(list(df['article_text']))\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "contemporary-science",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 1000\n",
    "n_components = len(set(df.category))\n",
    "n_top_words = 20 #number of top words to be extracted for a single topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "oriental-glucose",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def count_vectorize(articles):\n",
    "    \n",
    "    vectorizer = CountVectorizer(\n",
    "        max_df=0.95,\n",
    "        min_df=3, \n",
    "        max_features=n_features,\n",
    "        stop_words=stop_words_ru,\n",
    "        ngram_range=(1, 1))\n",
    "\n",
    "    token_count_matrix = vectorizer.fit_transform(corpus)\n",
    "    features = vectorizer.get_feature_names()\n",
    "        \n",
    "    vocab = open(\"./bigARTM/vocab.rbc.txt\", \"w\")\n",
    "    for feature in features:\n",
    "        vocab.write(feature+'\\n')\n",
    "    vocab.close()\n",
    "    \n",
    "    docword = open(\"./bigARTM/docword.rbc.txt\", \"w\")\n",
    "    cx = token_count_matrix.tocoo() #to coordinates\n",
    "    for docID, wordID, wordCount in zip(cx.row, cx.col, cx.data):\n",
    "        wordID += 1 # making it unity based to suit with bigARTM\n",
    "        docID += 1\n",
    "        docword.write(f\"{docID} {wordID} {wordCount}\\n\")\n",
    "    docword.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "loaded-interaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorize(articles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
