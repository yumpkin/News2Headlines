{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "burning-program",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/aliak/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/aliak/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk, razdel\n",
    "from pymystem3 import Mystem\n",
    "import string, regex as re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "# Download nltk packages used in this example\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "equivalent-sound",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"../dataset/rbc_2k_hashed_shuffled.json\")\n",
    "df = df.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "imported-binary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "politics     524\n",
       "economics    404\n",
       "finance      339\n",
       "business     338\n",
       "tech         233\n",
       "society      177\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "approximate-behalf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_sentences(article_text):\n",
    "    sentences = list()\n",
    "    for sentence in razdel.sentenize(article_text):\n",
    "        sentences.append(sentence.text)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "varying-buffalo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_tokens(article_sentences):\n",
    "    tokens = list()\n",
    "\n",
    "    for sentence in article_sentences:\n",
    "        for token in razdel.tokenize(sentence):\n",
    "            if token.text not in stop_words:\n",
    "                tokens.append(token.text.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "minimal-camel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_lemmas(article_sentences):\n",
    "    mystem = Mystem()\n",
    "    lemmas = list()\n",
    "    \n",
    "    for sentence in article_sentences:\n",
    "        sentence_lemmas = mystem.lemmatize(sentence.lower())\n",
    "        sentence_lemmas = [lemma for lemma in sentence_lemmas if lemma not in stop_words\\\n",
    "          and lemma != \" \"\\\n",
    "          and lemma.strip() not in punctuations]\n",
    "    lemmas+=sentence_lemmas\n",
    "    return set(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "secret-double",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize list of stopwords as needed. Here, we append common\n",
    "# punctuation and contraction artifacts.\n",
    "with open('../aux/stopwords-ru.txt', 'r') as f:\n",
    "    ru_stop_words_extensive = f.read().splitlines()\n",
    "    \n",
    "punctuations = list(string.punctuation) + [\"—\", \"«\", \"»\", \"\\n\"]\n",
    "stop_words = list(set(ru_stop_words_extensive + stopwords.words('russian'))) + punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "periodic-patch",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r\"[^а-яА-Я]\", \" \", doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    sentences = get_article_sentences(doc)\n",
    "    # tokenize document\n",
    "    lemmas = get_article_lemmas(sentences)\n",
    "    #filter stopwords out of document\n",
    "    tokens = get_article_tokens(sentences)\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(lemmas)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-statistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def calculate_cv_matrix(corpus, gram_count, min_df=10, max_df=0.009):\n",
    "    ngrams = {'only_unigrams':(1,1), 'unigrams_bigrams':(1,2), 'only_bigrams':(2,2), 'bigrams_trigrams':(2,3), 'only_trigrams':(3,3)}\n",
    "    cv = TfidfVectorizer(ngram_range=ngrams[gram_count], min_df=min_df, max_df=max_df, stop_words=stop_words)\n",
    "    cv_matrix = cv.fit_transform(corpus)\n",
    "    cv_matrix.shape\n",
    "    return cv_matrix, cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "sustained-valentine",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "def apply_kmeans(cv_matrix, cluster_count):\n",
    "    NUM_CLUSTERS = cluster_count\n",
    "    km = KMeans(n_clusters=NUM_CLUSTERS, max_iter=10000, n_init=100, random_state=32).fit(cv_matrix)\n",
    "    return km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "detailed-coalition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2015"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_corpus = normalize_corpus(list(df['article_text']))\n",
    "len(norm_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "tested-event",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aliak/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "cv_matrix, cv = calculate_cv_matrix(norm_corpus, 'only_unigrams', 10, 0.009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "loving-current",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 1310, 5: 65, 4: 76, 0: 404, 3: 135, 1: 25})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km = apply_kmeans(cv_matrix, 6)\n",
    "Counter(km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "great-geneva",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_clusters(km, cv, df, NUM_CLUSTERS):\n",
    "    df['kmeans_cluster'] = km.labels_\n",
    "    article_clusters = (df[['category', 'kmeans_cluster',]]\n",
    "                  .sort_values(by=['kmeans_cluster'], \n",
    "                               ascending=False)\n",
    "                  .groupby('kmeans_cluster').head(20))\n",
    "    article_clusters = article_clusters.copy(deep=True)\n",
    "    \n",
    "    feature_names = cv.get_feature_names()\n",
    "    topn_features = 15\n",
    "    ordered_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "    counts = dict()\n",
    "\n",
    "    # get key features for each cluster\n",
    "    # get articles belonging to each cluster\n",
    "    for cluster_num in range(NUM_CLUSTERS):\n",
    "        key_features = [feature_names[index] \n",
    "                            for index in ordered_centroids[cluster_num, :topn_features]]\n",
    "        articles = article_clusters[article_clusters['kmeans_cluster'] == cluster_num]['category'].values.tolist()\n",
    "        for category in articles:\n",
    "            counts[category] = counts.get(category, 0) + 1\n",
    "\n",
    "        counts = dict(sorted(counts.items(), key=lambda item: item[1], reverse=True))\n",
    "        print('CLUSTER #'+str(cluster_num+1))\n",
    "        print('Key Features:', key_features, \"\\n\")\n",
    "        print('Corresponding categories: \\n', counts)\n",
    "        print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "federal-attack",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLUSTER #1\n",
      "Key Features: ['роскосмос', 'согласование', 'односторонний', 'совкомбанк', 'ценовый', 'беспрецедентный', 'пошлина', 'африка', 'ран', 'держать', 'констатировать', 'послабление', 'ученый', 'абрамченко', 'фтс'] \n",
      "\n",
      "Corresponding categories: \n",
      " {'economics': 7, 'finance': 4, 'business': 3, 'tech': 3, 'politics': 3}\n",
      "--------------------------------------------------------------------------------\n",
      "CLUSTER #2\n",
      "Key Features: ['вкусно', 'франшиза', 'говор', 'весело', 'пушкинский', 'касса', 'логотип', 'юрлицо', 'гребенщиков', 'сооснователь', 'зеленый', 'антимонопольный', 'обратно', 'отель', 'фас'] \n",
      "\n",
      "Corresponding categories: \n",
      " {'business': 17, 'economics': 7, 'finance': 5, 'tech': 4, 'society': 4, 'politics': 3}\n",
      "--------------------------------------------------------------------------------\n",
      "CLUSTER #3\n",
      "Key Features: ['траст', 'роснано', 'безусловно', 'прокуратура', 'маск', 'ежегодный', 'потребкредитование', 'ускорять', 'райффайзенбанк', 'прирост', 'понести', 'обходиться', 'евробонд', 'взамен', 'задержание'] \n",
      "\n",
      "Corresponding categories: \n",
      " {'business': 18, 'economics': 11, 'finance': 9, 'politics': 9, 'society': 7, 'tech': 6}\n",
      "--------------------------------------------------------------------------------\n",
      "CLUSTER #4\n",
      "Key Features: ['увольнение', 'бедность', 'минтруд', 'возражать', 'претензия', 'превращаться', 'занятость', 'безработица', 'голикова', 'беженец', 'пособие', 'содействие', 'отзыв', 'пенсия', 'работодатель'] \n",
      "\n",
      "Corresponding categories: \n",
      " {'business': 20, 'economics': 19, 'politics': 14, 'finance': 13, 'society': 8, 'tech': 6}\n",
      "--------------------------------------------------------------------------------\n",
      "CLUSTER #5\n",
      "Key Features: ['эрдоган', 'антитеррористический', 'декларация', 'обозначать', 'сирия', 'иран', 'анкара', 'север', 'эскалация', 'желтый', 'разрушать', 'критический', 'личность', 'отходить', 'азербайджан'] \n",
      "\n",
      "Corresponding categories: \n",
      " {'politics': 25, 'economics': 23, 'business': 21, 'finance': 14, 'society': 10, 'tech': 7}\n",
      "--------------------------------------------------------------------------------\n",
      "CLUSTER #6\n",
      "Key Features: ['мобилизация', 'нрд', 'геноцид', 'всу', 'реактивный', 'залповый', 'киевский', 'выполняться', 'долгий', 'артиллерийский', 'донецк', 'поражать', 'снаряд', 'боеприпасы', 'ракетный'] \n",
      "\n",
      "Corresponding categories: \n",
      " {'politics': 39, 'economics': 27, 'business': 21, 'finance': 16, 'society': 10, 'tech': 7}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print_clusters(km, cv, df, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "brazilian-bridal",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('../dataset/pickles/clustered_articles.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-mainland",
   "metadata": {},
   "source": [
    "As we can see it does pretty good job, even for man-kind attempt, distinguishing between the categories (finance, business, economics) isn't that straightforward.\n",
    "It might be interesting to try clustering only articles from these categories and figure out how successful it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bigger-honduras",
   "metadata": {},
   "outputs": [],
   "source": [
    "money_related_df = df.loc[(df.category == \"finance\") | (df.category == \"business\") | (df.category == \"economics\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understood-heather",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_corpus = normalize_corpus(list(money_related_df['article_overview']))\n",
    "cv_matrix, cv = calculate_cv_matrix(norm_corpus, 'only_bigrams', min_df=20, max_df=0.009)\n",
    "NUM_CLUSTERS = 3\n",
    "km = apply_kmeans(cv_matrix, NUM_CLUSTERS)\n",
    "Counter(km.labels_)\n",
    "print_clusters(km, cv, money_related_df,NUM_CLUSTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-emergency",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_corpus = normalize_corpus(list(money_related_df['article_text']))\n",
    "cv_matrix, cv = calculate_cv_matrix(norm_corpus, 'only_trigrams', min_df=10, max_df=0.009)\n",
    "NUM_CLUSTERS = 3\n",
    "km = apply_kmeans(cv_matrix, NUM_CLUSTERS)\n",
    "Counter(km.labels_)\n",
    "print_clusters(km, cv, money_related_df,NUM_CLUSTERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-literacy",
   "metadata": {},
   "source": [
    "#### We can notice how the results would get a better divergence as the window size of ngrams exapands, and the min-occurencing frequency increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-career",
   "metadata": {},
   "outputs": [],
   "source": [
    "social_related_df = df.loc[(df.category != \"finance\") | (df.category != \"business\") | (df.category != \"economics\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-convertible",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_corpus = normalize_corpus(list(social_related_df['article_overview']))\n",
    "cv_matrix, cv = calculate_cv_matrix(norm_corpus, 'only_bigrams')\n",
    "NUM_CLUSTERS = 3\n",
    "km = apply_kmeans(cv_matrix, NUM_CLUSTERS)\n",
    "Counter(km.labels_)\n",
    "print_clusters(km, cv, money_related_df,NUM_CLUSTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-extent",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_corpus = normalize_corpus(list(social_related_df['article_text']))\n",
    "cv_matrix, cv = calculate_cv_matrix(norm_corpus, 'only_trigrams', 3)\n",
    "NUM_CLUSTERS = 3\n",
    "km = apply_kmeans(cv_matrix, NUM_CLUSTERS)\n",
    "Counter(km.labels_)\n",
    "print_clusters(km, cv, money_related_df,NUM_CLUSTERS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
