{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "running-muslim",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/aliak/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/aliak/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk, razdel\n",
    "from pymystem3 import Mystem\n",
    "import string, regex as re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "# Download nltk packages used in this example\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "innocent-independence",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"../dataset/enumerated_shuffled_rbc_dataset.json\")\n",
    "df = df.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "electronic-dominant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "finance      339\n",
       "business     338\n",
       "economics    261\n",
       "politics     258\n",
       "tech         233\n",
       "society      177\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "trained-toyota",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_sentences(article_text):\n",
    "    sentences = list()\n",
    "    for sentence in razdel.sentenize(article_text):\n",
    "        sentences.append(sentence.text)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "occupied-capital",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_tokens(article_sentences):\n",
    "    tokens = list()\n",
    "\n",
    "    for sentence in article_sentences:\n",
    "        for token in razdel.tokenize(sentence):\n",
    "            if token.text not in stop_words:\n",
    "                tokens.append(token.text.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "instructional-doubt",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_lemmas(article_sentences):\n",
    "    mystem = Mystem()\n",
    "    lemmas = list()\n",
    "    \n",
    "    for sentence in article_sentences:\n",
    "        sentence_lemmas = mystem.lemmatize(sentence.lower())\n",
    "        sentence_lemmas = [lemma for lemma in sentence_lemmas if lemma not in stop_words\\\n",
    "          and lemma != \" \"\\\n",
    "          and lemma.strip() not in punctuations]\n",
    "    lemmas+=sentence_lemmas\n",
    "    return set(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "concrete-performer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize list of stopwords as needed. Here, we append common\n",
    "# punctuation and contraction artifacts.\n",
    "with open('../aux/stopwords-ru.txt', 'r') as f:\n",
    "    ru_stop_words_extensive = f.read().splitlines()\n",
    "    \n",
    "punctuations = list(string.punctuation) + [\"—\", \"«\", \"»\", \"\\n\"]\n",
    "stop_words = list(set(ru_stop_words_extensive + stopwords.words('russian'))) + punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ready-thanksgiving",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r\"[^а-яА-Я]\", \" \", doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    sentences = get_article_sentences(doc)\n",
    "    # tokenize document\n",
    "    lemmas = get_article_lemmas(sentences)\n",
    "    #filter stopwords out of document\n",
    "    tokens = get_article_tokens(sentences)\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(lemmas)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "british-amazon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def calculate_cv_matrix(corpus, gram_count, min_df=0.5, max_df=1):\n",
    "    ngrams = {'only_unigrams':(1,1), 'unigrams_bigrams':(1,2), 'only_bigrams':(2,2), 'bigrams_trigrams':(2,3), 'only_trigrams':(3,3)}\n",
    "    cv = CountVectorizer(ngram_range=ngrams[gram_count], min_df=min_df, max_df=max_df, stop_words=stop_words)\n",
    "    cv_matrix = cv.fit_transform(corpus)\n",
    "    cv_matrix.shape\n",
    "    return cv_matrix, cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "alleged-brake",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "def apply_kmeans(cv_matrix, cluster_count):\n",
    "    NUM_CLUSTERS = cluster_count\n",
    "    km = KMeans(n_clusters=NUM_CLUSTERS, max_iter=10000, n_init=100, random_state=32).fit(cv_matrix)\n",
    "    return km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fourth-rebate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1606"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_corpus = normalize_corpus(list(df['article_text']))\n",
    "len(norm_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-freeware",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "cv_matrix, cv = calculate_cv_matrix(norm_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "regular-layout",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 447, 4: 918, 1: 236, 3: 1, 0: 2, 5: 2})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km = apply_kmeans(cv_matrix, 6)\n",
    "Counter(km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "lined-canal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_clusters(km, cv, df, NUM_CLUSTERS):\n",
    "    df['kmeans_cluster'] = km.labels_\n",
    "    article_clusters = (df[['category', 'kmeans_cluster',]]\n",
    "                  .sort_values(by=['kmeans_cluster'], \n",
    "                               ascending=False)\n",
    "                  .groupby('kmeans_cluster').head(20))\n",
    "    article_clusters = article_clusters.copy(deep=True)\n",
    "    \n",
    "    feature_names = cv.get_feature_names()\n",
    "    topn_features = 15\n",
    "    ordered_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "    counts = dict()\n",
    "\n",
    "    # get key features for each cluster\n",
    "    # get articles belonging to each cluster\n",
    "    for cluster_num in range(NUM_CLUSTERS):\n",
    "        key_features = [feature_names[index] \n",
    "                            for index in ordered_centroids[cluster_num, :topn_features]]\n",
    "        articles = article_clusters[article_clusters['kmeans_cluster'] == cluster_num]['category'].values.tolist()\n",
    "        for category in articles:\n",
    "            counts[category] = counts.get(category, 0) + 1\n",
    "\n",
    "        counts = dict(sorted(counts.items(), key=lambda item: item[1], reverse=True))\n",
    "        print('CLUSTER #'+str(cluster_num+1))\n",
    "        print('Key Features:', key_features, \"\\n\")\n",
    "        print('Corresponding categories: \\n', counts)\n",
    "        print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "competitive-growth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLUSTER #1\n",
      "Key Features: ['технический', 'тыс baring', 'соловьев', 'подросток', 'счет', 'пятачок рбк', 'пятачок', 'оставаться', 'взрослый', 'счет венчурный', 'взрослый топ', 'принцип сад', 'определять', 'сделка', 'тыс'] \n",
      "\n",
      "Corresponding categories: \n",
      " {'tech': 2}\n",
      "--------------------------------------------------------------------------------\n",
      "CLUSTER #2\n",
      "Key Features: ['www', 'adv', 'rbc', 'ru', 'составлять', 'rbc ru', 'рынок', 'данные', 'российский', 'начало', 'рост', 'компания', 'отмечать', 'цена', '2021'] \n",
      "\n",
      "Corresponding categories: \n",
      " {'economics': 9, 'business': 4, 'society': 4, 'tech': 3, 'politics': 1, 'finance': 1}\n",
      "--------------------------------------------------------------------------------\n",
      "CLUSTER #3\n",
      "Key Features: ['ru', 'rbc', 'adv', 'www', 'rbc ru', 'сообщать', 'российский', 'компания', 'заявлять', 'президент', 'становиться', 'рбк', 'данные', 'начало', 'число'] \n",
      "\n",
      "Corresponding categories: \n",
      " {'economics': 10, 'business': 10, 'society': 8, 'politics': 7, 'tech': 6, 'finance': 1}\n",
      "--------------------------------------------------------------------------------\n",
      "CLUSTER #4\n",
      "Key Features: ['министр', 'опек', 'смочь', 'агентство иностранный', 'тасс', 'продажа', 'агентство', 'дальнейший декабрь', 'дальнейший', 'соглашение аналитика', 'экспортер', 'соглашение', 'скорректировать', 'уменьшаться', 'заявлять стратегический'] \n",
      "\n",
      "Corresponding categories: \n",
      " {'economics': 11, 'business': 10, 'society': 8, 'politics': 7, 'tech': 6, 'finance': 1}\n",
      "--------------------------------------------------------------------------------\n",
      "CLUSTER #5\n",
      "Key Features: ['российский', 'компания', 'отмечать', 'сообщать', 'рбк', 'банк', 'заявлять', 'май', 'исследование', 'магазин', 'глава', 'тема', 'президент', 'украина', 'становиться'] \n",
      "\n",
      "Corresponding categories: \n",
      " {'economics': 14, 'business': 12, 'politics': 11, 'tech': 11, 'society': 9, 'finance': 6}\n",
      "--------------------------------------------------------------------------------\n",
      "CLUSTER #6\n",
      "Key Features: ['пытка', '2016', 'ставить', 'ставить жалоба', 'силовик', '2016 признание', 'сообщать заявление', 'юрлицо', 'юрлица', 'млн политика', 'проверять', 'проверять разногласие', '2015', 'интерес предотвращение', 'интерес'] \n",
      "\n",
      "Corresponding categories: \n",
      " {'economics': 14, 'business': 12, 'politics': 12, 'tech': 11, 'society': 10, 'finance': 6}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print_clusters(km, cv, df, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "united-perfume",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('../dataset/pickles/clustered_articles.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposed-china",
   "metadata": {},
   "source": [
    "As we can see it does pretty good job, even for man-kind attempt, distinguishing between the categories (finance, business, economics) isn't that straightforward.\n",
    "It might be interesting to try clustering only articles from these categories and figure out how successful it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "inclusive-transsexual",
   "metadata": {},
   "outputs": [],
   "source": [
    "money_related_df = df.loc[(df.category == \"finance\") | (df.category == \"business\") | (df.category == \"economics\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "reported-organic",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aliak/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLUSTER #1\n",
      "Key Features: ['снижать цб', 'поставка компания', 'компания российский', 'цб кредит', 'рассказывать рбк', 'российский бизнес', 'риск рбк', 'банк эксперт', 'ставка руб', 'компания бизнес', 'продажа компания', 'цена достигать', 'санкция сообщать', 'втб банк', 'ес решение'] \n",
      "\n",
      "Corresponding categories: \n",
      " {'economics': 9, 'business': 7, 'finance': 4}\n",
      "--------------------------------------------------------------------------------\n",
      "CLUSTER #2\n",
      "Key Features: ['поставка российский', 'ес решение', 'поставка компания', 'банк эксперт', 'банка банк', 'втб банк', 'газпром компания', 'данные сша', 'инфляция рост', 'компания бизнес', 'компания российский', 'малый бизнес', 'нефть рынок', 'объем млрд', 'пандемия фон'] \n",
      "\n",
      "Corresponding categories: \n",
      " {'economics': 17, 'business': 12, 'finance': 4}\n",
      "--------------------------------------------------------------------------------\n",
      "CLUSTER #3\n",
      "Key Features: ['российский компания', 'цена достигать', 'пандемия фон', 'объем млрд', 'нефть рынок', 'малый бизнес', 'компания российский', 'компания бизнес', 'инфляция рост', 'поставка компания', 'ес решение', 'данные сша', 'газпром компания', 'втб банк', 'банка банк'] \n",
      "\n",
      "Corresponding categories: \n",
      " {'economics': 22, 'business': 20, 'finance': 6}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aliak/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:3607: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._set_item(key, value)\n"
     ]
    }
   ],
   "source": [
    "norm_corpus = normalize_corpus(list(money_related_df['article_overview']))\n",
    "cv_matrix, cv = calculate_cv_matrix(norm_corpus, 'only_bigrams')\n",
    "NUM_CLUSTERS = 3\n",
    "km = apply_kmeans(cv_matrix, NUM_CLUSTERS)\n",
    "Counter(km.labels_)\n",
    "print_clusters(km, cv, money_related_df,NUM_CLUSTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "applied-neighborhood",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aliak/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLUSTER #1\n",
      "Key Features: ['принадлежать rbc ru', 'находиться принадлежать rbc', 'неустойка информация суд', 'указывать принадлежать rbc', 'санация принадлежать rbc', 'rbc ru объединять', 'втб акция банк', 'центробанк финансовый собеседник', 'начало банкротство мера', 'руб ограничение покидать', 'признание 2020 продавать', 'крупный adv возможность', 'российский компания имущество', 'финансовый собеседник означать', 'фон директор рбк'] \n",
      "\n",
      "Corresponding categories: \n",
      " {'business': 9, 'finance': 5, 'economics': 2}\n",
      "--------------------------------------------------------------------------------\n",
      "CLUSTER #2\n",
      "Key Features: ['rbc ru власть', 'ru власть санкция', 'находиться rbc ru', 'начинать оказываться заявлять', 'срок rbc ru', 'экономика российский компания', 'рамка пресс служба', 'торговаться rbc ru', 'российский компания 20', 'вернуть денежный российский', 'пакет руб апрель', 'повышать цена достигать', 'повышать прошлый цена', 'переставать являться насколько', 'отмечать упасть нефть'] \n",
      "\n",
      "Corresponding categories: \n",
      " {'business': 15, 'finance': 12, 'economics': 9}\n",
      "--------------------------------------------------------------------------------\n",
      "CLUSTER #3\n",
      "Key Features: ['поставка компания российский', 'rbc ru квартал', 'rbc ru санкция', 'банка аналитика исследование', 'накануне rbc ru', 'начало аналитика исследование', 'поставка российский компания', 'компания российский бизнес', 'находиться rbc ru', 'большинство rbc ru', 'rbc ru инвестор', 'www иностранный сообщать', 'начало президент поставлять', 'срок rbc ru', 'www агентство сообщать'] \n",
      "\n",
      "Corresponding categories: \n",
      " {'economics': 21, 'business': 19, 'finance': 16}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aliak/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:3607: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._set_item(key, value)\n"
     ]
    }
   ],
   "source": [
    "norm_corpus = normalize_corpus(list(money_related_df['article_text']))\n",
    "cv_matrix, cv = calculate_cv_matrix(norm_corpus, 'only_trigrams', 3)\n",
    "NUM_CLUSTERS = 3\n",
    "km = apply_kmeans(cv_matrix, NUM_CLUSTERS)\n",
    "Counter(km.labels_)\n",
    "print_clusters(km, cv, money_related_df,NUM_CLUSTERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-policy",
   "metadata": {},
   "source": [
    "#### We can notice how the results would get a better divergence as the window size of ngrams exapands, and the min-occurencing frequency increases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
