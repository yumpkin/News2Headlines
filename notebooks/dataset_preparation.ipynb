{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "through-beach",
   "metadata": {},
   "source": [
    "This notebook is dedicated to parse RBC website and extract various articles, from the main 6 categories:\n",
    "* Politics\n",
    "* Social\n",
    "* Tech\n",
    "* Business\n",
    "* Finance\n",
    "* Economy\n",
    "\n",
    "Then the results are saved into 2 formats: `JSON` and `txt`\n",
    "* `JSON` files, are either indexed by the hash of the article text itsself, for a more convient checkup and later operational matchings. Or they are indexed by the enumeration of the articles, starting form `0` ending with `Total count of articles-1`. Both also include for each article its `category`, `article_overview` and auxiliary `tags` for extra more info.\n",
    "* `Txt` files on the other hand, made for faster dynamic separtion and uploads. They are as follows: One file for all headlines, separated by a new line and a special charachter(to avoid mixing them up while traversing them). The second file dedicated for all body text of the articles, and the last one is for all overview(annotation at the top of the articles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "wrapped-sudan",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import json\n",
    "import hashlib\n",
    "import os\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "pleased-attribute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 100.0.4896\n",
      "[WDM] - Get LATEST chromedriver version for 100.0.4896 google-chrome\n",
      "[WDM] - Driver [/home/aliak/.wdm/drivers/chromedriver/linux64/100.0.4896.60/chromedriver] found in cache\n"
     ]
    }
   ],
   "source": [
    "options = Options()\n",
    "options.add_argument(\"start-maximized\")\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "patient-diesel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "\n",
    "def parse(link):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "    session = requests.Session()\n",
    "    retry = Retry(connect=3, backoff_factor=0.5)\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    session.get(link, headers=headers)\n",
    "    response = session.get(link, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "joint-coast",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_To_json(file_name, entry):\n",
    "    try :\n",
    "        with open(file_name) as json_file:\n",
    "            data = json.load(json_file)\n",
    "    except :\n",
    "        data = {}\n",
    "\n",
    "    data.update(entry)\n",
    "\n",
    "    with open(file_name, \"w+\") as write_file:\n",
    "        json.dump(data, write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "concrete-compilation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_with_hash(dict_entry):\n",
    "    try:\n",
    "        return {hashlib.md5(dict_entry['article_text'].encode()).hexdigest(): dict_entry}\n",
    "    except Exception as e:\n",
    "        print(e, dict_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "acute-elephant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_page(category, url, file_name):\n",
    "    driver.get(url)\n",
    "    time.sleep(2)  # Allow 2 seconds for the web page to open\n",
    "    scroll_pause_time = 1 # You can set your own pause time. My laptop is a bit slow so I use 1 sec\n",
    "    screen_height = driver.execute_script(\"return window.screen.height;\")   # get the screen height of the web\n",
    "    i = 1\n",
    "\n",
    "    while True:\n",
    "        # scroll one screen height each time\n",
    "        driver.execute_script(\"window.scrollTo(0, {screen_height}*{i});\".format(screen_height=screen_height, i=i))  \n",
    "        i += 1\n",
    "        time.sleep(scroll_pause_time)\n",
    "        # update scroll height each time after scrolled, as the scroll height can change after we scrolled the page\n",
    "        scroll_height = driver.execute_script(\"return document.body.scrollHeight;\")  \n",
    "        # Break the loop when the height we need to scroll to is larger than the total scroll height\n",
    "        if (screen_height) * i > scroll_height:\n",
    "            break \n",
    "\n",
    "        \n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    articles = {}\n",
    "\n",
    "    for element in soup.body.findAll('div', attrs={'class': 'item item_image-mob js-category-item'}):\n",
    "        articles['url'] = element.find('a').get('href')\n",
    "        articles['date'] = element.select_one('span.item__category').text.strip()\n",
    "        articles['title'] = element.select_one('span.rm-cm-item-text').text.strip()\n",
    "        articles['category'] = category\n",
    "\n",
    "        soup = parse(articles['url'])\n",
    "        corpus = {}\n",
    "        trash = ['article__main-image', 'article__header__info-block', 'pro-anons', 'article__authors', 'social-networks__content', 'article__inline-item']\n",
    "\n",
    "        for text_div in soup.findAll('div', attrs={'class':'l-col-main'}):\n",
    "            extracted_trash = [t.extract() for trash_class in trash for t in text_div.findAll('div', attrs={'class':trash_class})]\n",
    "            del extracted_trash\n",
    "\n",
    "            for tag_bar in text_div.findAll('div', attrs={'class':'article__tags__container'}):\n",
    "                tags = [tag.string for tag in tag_bar.findAll('a') if tag_bar is not None ]\n",
    "\n",
    "            headline = text_div.find('h1')\n",
    "            overview = text_div.find('div', attrs = {'article__text__overview'})\n",
    "            if text_div is not None and overview is not None and headline is not None and tags is not None:\n",
    "                corpus['headline'] = str(headline.text).strip()\n",
    "                corpus['article_overview'] = overview.span.text\n",
    "                corpus['category'] = category\n",
    "                corpus['tags'] = tags\n",
    "                tag_bar.extract()\n",
    "                corpus['article_text'] = (re.sub(r'\\s+', ' ', text_div.text)).replace('Авторы Теги', '')\n",
    "                corpus['article_text'] = re.sub(r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))', '', corpus['article_text'])\n",
    "            else:\n",
    "                continue\n",
    "        if bool(corpus):        \n",
    "            entry = index_with_hash(corpus)\n",
    "        try :\n",
    "            with open(file_name) as json_file:\n",
    "                data = json.load(json_file)\n",
    "        except :\n",
    "            data = {}\n",
    "            \n",
    "        data.update(entry)\n",
    "\n",
    "        with open(file_name, \"w+\") as write_file:\n",
    "            json.dump(data, write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alike-cloud",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "link_to_parse = {}\n",
    "link_to_parse['politics'] = 'https://www.rbc.ru/politics/?utm_source=topline'\n",
    "link_to_parse['economics'] = 'https://www.rbc.ru/economics/?utm_source=topline'\n",
    "link_to_parse['society'] = 'https://www.rbc.ru/society/?utm_source=topline'\n",
    "link_to_parse['business'] = 'https://www.rbc.ru/business/?utm_source=topline'\n",
    "link_to_parse['tech'] = 'https://www.rbc.ru/technology_and_media/?utm_source=topline'\n",
    "link_to_parse['finance'] = 'https://www.rbc.ru/finances/?utm_source=topline'\n",
    "\n",
    "\n",
    "file_name = '../dataset/hashed_shuffled_rbc.json'\n",
    "for category, link in link_to_parse.items():\n",
    "    parse_page(category, link, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "nonprofit-marshall",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "df = pd.read_json('../dataset/hashed_shuffled_rbc.json')\n",
    "df = df.transpose()\n",
    "df = shuffle(df)\n",
    "df.to_json('../dataset/hashed_shuffled_rbc.json', orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dying-belly",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enumerated = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "vertical-spyware",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enumerated.to_json('../dataset/enumerated_shuffled_rbc.json', orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-mercury",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_textfiles(dataset='../dataset/enumerated_shuffled_rbc.json', headlines_txt_path='../dataset/rbc_only_headlines.txt', annotation_txt_path='../dataset/rbc_only_annotation.txt', article_body_txt_path='../dataset/rbc_only_body.txt'):\n",
    "    df = pd.read_json(dataset)\n",
    "    df = df.transpose()\n",
    "    special_separator = '<@$>\\n\\n'\n",
    "    with open(headlines_txt_path, 'w', encoding = 'utf-8') as h, open(annotation_txt_path, 'w', encoding = 'utf-8') as a, open(article_body_txt_path, 'w', encoding = 'utf-8') as b:\n",
    "        for _, rec in df.iterrows():\n",
    "            h.write(rec['headline'] + special_separator)\n",
    "            a.write(rec['article_overview'] + special_separator)\n",
    "            b.write(rec['article_text'] + special_separator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "binary-diameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "smart-horse",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('../dataset/rbc_2k.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "under-newsletter",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "backed-cathedral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEUscore: пресс-конференция владимира путина   в столичном манеже прошла пресс-конференция президента россии владимира путина 0.4037500002676021\n",
      "BLEUscore: пресс-конференция владимира путина   в московском манеже прошла ежегодная пресс-конференция владимира путина 0.45433186333194825\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate import bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "smoothie = SmoothingFunction().method4\n",
    "\n",
    "for i in range(len(df)):\n",
    "    headline = df['headline'][0].lower()\n",
    "    headline = headline.split('.')[0]\n",
    "    text = df['article_text'][i]\n",
    "    \n",
    "    C1= headline\n",
    "    C2= text.split('.')[0].lower()\n",
    "    score = bleu([C1], C2, smoothing_function=smoothie)\n",
    "    if score > 0.4:\n",
    "        print('BLEUscore:', C1, C2, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-surge",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "heard-vulnerability",
   "metadata": {},
   "source": [
    "### Ria Pre-proccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "sunset-writing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ria_records(file_name, dataset_size, shuffle=False):\n",
    "    records = []\n",
    "    count = 0\n",
    "    with open(file_name, \"r\", buffering=100000) as r:\n",
    "        for line in r:\n",
    "            records.append(json.loads(line))\n",
    "            count+=1\n",
    "            if count == dataset_size:\n",
    "                break\n",
    "    if shuffle:\n",
    "        random.shuffle(records)\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "indian-rhythm",
   "metadata": {},
   "outputs": [],
   "source": [
    "ria_records = read_ria_records('../dataset/full_ria.json', 2000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "female-uruguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "ria_df = pd.DataFrame.from_records(ria_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "young-scientist",
   "metadata": {},
   "outputs": [],
   "source": [
    "ria_df = ria_df.iloc[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "confidential-rendering",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p><strong></strong></p>\\n<p><strong>киев, 31 янв - риа новости, марина шмаюн.</strong> премьер-министр украины, кандидат в президенты юлия тимошенко в воскресенье в прямом эфире <a href=\"http://www.1plus1.ua/\" target=\"_blank\">украинского телеканала 1+1</a> заявила, что в случае ее победы на выборах президента юрий луценко будет работать в ее команде.</p>\\n<p>17 января в украине состоялся первый тур выборов президента, по итогам которого <a href=\"http://rian.ru/politics/20100125/206201353.html\" target=\"_blank\">виктор янукович набрал 35,32% голосов, а премьер-министр страны юлия тимошенко оказалась на втором месте с результатом 25,05%</a>. второй тур выборов президента украины <a href=\"http://www.rian.ru/trend/ukraine_election_18012010/\" target=\"_blank\">состоится 7 февраля</a>.</p>\\n<p>парламент украины по инициативе партии регионов 28 января <a href=\"http://rian.ru/trend/ukr_lutsenko_retired_28012010/\" target=\"_blank\">отправил в отставку главу мвд юрия луценко</a>, однако премьер-министр юлия тимошенко <a href=\"http://rian.ru/international_justice/20100129/206834215.html\" target=\"_blank\">внесла в кабмин его кандидатуру на утверждение первым замминистра внутренних дел</a> с полномочиями и.о. министра. правительство единогласно поддержало это предложение. партия регионов оспорила это решение кабмина в суде.</p>\\n<p>\"что касается новых назначений, то знаете, как народная мудрость гласит: не нужно говорить \"гоп\", пока еще не произошло событие. потому я считаю, что нужно сначала выиграть выборы, обсудить все вопросы создания новой команды, и я убеждена, что юрий луценко в команде будет работать\", - сказала тимошенко.</p>'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ria_df['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "opening-concentration",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "def remove_html_tags(text):\n",
    "    text_string = BeautifulSoup(text, \"lxml\").text\n",
    "    clean_text = unicodedata.normalize(\"NFKD\",text_string).replace('\\n','')\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "computational-cowboy",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_titles = [\"title\",\"text\"]\n",
    "ria_df=ria_df.reindex(columns=columns_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "stainless-candidate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_date(text):\n",
    "    clean_text = '.'.join(text.split('.')[1:])\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "disciplinary-merchant",
   "metadata": {},
   "outputs": [],
   "source": [
    "ria_df['text'] = ria_df['text'].apply(func = remove_html_tags)\n",
    "ria_df['text'] = ria_df['text'].apply(func = clean_date)\n",
    "ria_df['title'] = ria_df['title'].apply(func = remove_html_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "little-truck",
   "metadata": {},
   "outputs": [],
   "source": [
    "ria_df.to_json('../dataset/ria_2k.json',orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-favorite",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "southeast-lightning",
   "metadata": {},
   "source": [
    "### Gazeta Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "compact-discretion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gazeta_records(file_name, shuffle=False, sort_by_date=True):\n",
    "    assert shuffle != sort_by_date\n",
    "    records = []\n",
    "    with open(file_name, \"r\") as r:\n",
    "        for line in r:\n",
    "            records.append(json.loads(line))\n",
    "    if sort_by_date:\n",
    "        records.sort(key=lambda x: x[\"date\"])\n",
    "    if shuffle:\n",
    "        random.shuffle(records)\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dressed-luther",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_records = read_gazeta_records(\"../dataset/gazeta_test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "formal-cisco",
   "metadata": {},
   "outputs": [],
   "source": [
    "gazeta_df = pd.DataFrame.from_records(test_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "industrial-utility",
   "metadata": {},
   "outputs": [],
   "source": [
    "gazeta_df = gazeta_df.iloc[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classified-taiwan",
   "metadata": {},
   "outputs": [],
   "source": [
    "gazeta_df.to_json('../dataset/gazeta_2k.json',orient='index')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
