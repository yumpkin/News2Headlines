{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dynamic-conviction",
   "metadata": {},
   "source": [
    "This notebook is dedicated to parse RBC website and extract various articles, from the main 6 categories:\n",
    "* Politics\n",
    "* Social\n",
    "* Tech\n",
    "* Business\n",
    "* Finance\n",
    "* Economy\n",
    "\n",
    "Then the results are saved into 2 formats: `JSON` and `txt`\n",
    "* `JSON` files, are either indexed by the hash of the article text itsself, for a more convient checkup and later operational matchings. Or they are indexed by the enumeration of the articles, starting form `0` ending with `Total count of articles-1`. Both also include for each article its `category`, `article_overview` and auxiliary `tags` for extra more info.\n",
    "* `Txt` files on the other hand, made for faster dynamic separtion and uploads. They are as follows: One file for all headlines, separated by a new line and a special charachter(to avoid mixing them up while traversing them). The second file dedicated for all body text of the articles, and the last one is for all overview(annotation at the top of the articles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "greek-ghost",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import json\n",
    "import hashlib\n",
    "import os\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "lightweight-sleeping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 100.0.4896\n",
      "[WDM] - Get LATEST chromedriver version for 100.0.4896 google-chrome\n",
      "[WDM] - About to download new driver from https://chromedriver.storage.googleapis.com/100.0.4896.60/chromedriver_linux64.zip\n",
      "[WDM] - Driver has been saved in cache [/home/aliak/.wdm/drivers/chromedriver/linux64/100.0.4896.60]\n"
     ]
    }
   ],
   "source": [
    "options = Options()\n",
    "options.add_argument(\"start-maximized\")\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "worst-shooting",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "\n",
    "def parse(link):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "    session = requests.Session()\n",
    "    retry = Retry(connect=3, backoff_factor=0.5)\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    session.get(link, headers=headers)\n",
    "    response = session.get(link, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "occupational-auckland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_To_json(file_name, entry):\n",
    "    try :\n",
    "        with open(file_name) as json_file:\n",
    "            data = json.load(json_file)\n",
    "    except :\n",
    "        data = {}\n",
    "\n",
    "    data.update(entry)\n",
    "\n",
    "    with open(file_name, \"w+\") as write_file:\n",
    "        json.dump(data, write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "driven-panic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_with_hash(dict_entry):\n",
    "    try:\n",
    "        return {hashlib.md5(dict_entry['article_text'].encode()).hexdigest(): dict_entry}\n",
    "    except Exception as e:\n",
    "        print(e, dict_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "metric-academy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_page(category, url, file_name):\n",
    "    driver.get(url)\n",
    "    time.sleep(2)  # Allow 2 seconds for the web page to open\n",
    "    scroll_pause_time = 1 # You can set your own pause time. My laptop is a bit slow so I use 1 sec\n",
    "    screen_height = driver.execute_script(\"return window.screen.height;\")   # get the screen height of the web\n",
    "    i = 1\n",
    "\n",
    "    while True:\n",
    "        # scroll one screen height each time\n",
    "        driver.execute_script(\"window.scrollTo(0, {screen_height}*{i});\".format(screen_height=screen_height, i=i))  \n",
    "        i += 1\n",
    "        time.sleep(scroll_pause_time)\n",
    "        # update scroll height each time after scrolled, as the scroll height can change after we scrolled the page\n",
    "        scroll_height = driver.execute_script(\"return document.body.scrollHeight;\")  \n",
    "        # Break the loop when the height we need to scroll to is larger than the total scroll height\n",
    "        if (screen_height) * i > scroll_height:\n",
    "            break \n",
    "\n",
    "        \n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    articles = {}\n",
    "\n",
    "    for element in soup.body.findAll('div', attrs={'class': 'item item_image-mob js-category-item'}):\n",
    "        articles['url'] = element.find('a').get('href')\n",
    "        articles['date'] = element.select_one('span.item__category').text.strip()\n",
    "        articles['title'] = element.select_one('span.rm-cm-item-text').text.strip()\n",
    "        articles['category'] = category\n",
    "\n",
    "        soup = parse(articles['url'])\n",
    "        corpus = {}\n",
    "        trash = ['article__main-image', 'article__header__info-block', 'pro-anons', 'article__authors', 'social-networks__content', 'article__inline-item']\n",
    "\n",
    "        for text_div in soup.findAll('div', attrs={'class':'l-col-main'}):\n",
    "            extracted_trash = [t.extract() for trash_class in trash for t in text_div.findAll('div', attrs={'class':trash_class})]\n",
    "            del extracted_trash\n",
    "\n",
    "            for tag_bar in text_div.findAll('div', attrs={'class':'article__tags__container'}):\n",
    "                tags = [tag.string for tag in tag_bar.findAll('a') if tag_bar is not None ]\n",
    "\n",
    "            headline = text_div.find('h1')\n",
    "            overview = text_div.find('div', attrs = {'article__text__overview'})\n",
    "            if text_div is not None and overview is not None and headline is not None and tags is not None:\n",
    "                corpus['headline'] = str(headline.text).strip()\n",
    "                corpus['article_overview'] = overview.span.text\n",
    "                corpus['category'] = category\n",
    "                corpus['tags'] = tags\n",
    "                tag_bar.extract()\n",
    "                corpus['article_text'] = (re.sub(r'\\s+', ' ', text_div.text)).str.replace('Авторы Теги', '')\n",
    "                corpus['article_text'] = (re.sub(r'\\s+', ' ', text_div.text)).str.replace(corpus['headline'], '') #most of RBC articles start with the headline\n",
    "                \n",
    "            else:\n",
    "                continue\n",
    "        if bool(corpus):        \n",
    "            entry = index_with_hash(corpus)\n",
    "        try :\n",
    "            with open(file_name) as json_file:\n",
    "                data = json.load(json_file)\n",
    "        except :\n",
    "            data = {}\n",
    "            \n",
    "        data.update(entry)\n",
    "\n",
    "        with open(file_name, \"w+\") as write_file:\n",
    "            json.dump(data, write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "express-track",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37min 18s, sys: 3min 28s, total: 40min 46s\n",
      "Wall time: 1h 6min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "link_to_parse = {}\n",
    "link_to_parse['politics'] = 'https://www.rbc.ru/politics/?utm_source=topline'\n",
    "link_to_parse['economics'] = 'https://www.rbc.ru/economics/?utm_source=topline'\n",
    "link_to_parse['society'] = 'https://www.rbc.ru/society/?utm_source=topline'\n",
    "link_to_parse['business'] = 'https://www.rbc.ru/business/?utm_source=topline'\n",
    "link_to_parse['tech'] = 'https://www.rbc.ru/technology_and_media/?utm_source=topline'\n",
    "link_to_parse['finance'] = 'https://www.rbc.ru/finances/?utm_source=topline'\n",
    "\n",
    "\n",
    "file_name = '../dataset/hashed_rbc.json'\n",
    "for category, link in link_to_parse.items():\n",
    "    parse_page(category, link, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "immune-telescope",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "df = pd.read_json('../dataset/hashed_rbc.json')\n",
    "df = df.transpose()\n",
    "df = shuffle(df)\n",
    "df.to_json('../dataset/hashed_shuffled_rbc.json', orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "nuclear-school",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_textfiles(dataset='../dataset/enumerated_shuffled_rbc.json', headlines_txt_path='../dataset/rbc_only_headlines.txt', annotation_txt_path='../dataset/rbc_only_annotation.txt', article_body_txt_path='../dataset/rbc_only_body.txt'):\n",
    "    df = pd.read_json(dataset)\n",
    "    df = df.transpose()\n",
    "    special_separator = '<@$>\\n\\n'\n",
    "    with open(headlines_txt_path, 'w', encoding = 'utf-8') as h, open(annotation_txt_path, 'w', encoding = 'utf-8') as a, open(article_body_txt_path, 'w', encoding = 'utf-8') as b:\n",
    "        for _, rec in df.iterrows():\n",
    "            h.write(rec['headline'] + special_separator)\n",
    "            a.write(rec['article_overview'] + special_separator)\n",
    "            b.write(rec['article_text'] + special_separator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "suspended-cardiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_to_textfiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-panel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historical-rebound",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unlike-armenia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "addressed-pathology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_json('../dataset/hashed_shuffled_rbc.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "motivated-custody",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "killing-rolling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# article = list(df['article_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "spoken-proposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# headline = list(df['headline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ecological-objective",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = list(zip(headline, article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-range",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "illegal-anime",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(df)):\n",
    "#     headline = df['headline'][i]\n",
    "#     df['article_text'][i] = df['article_text'][i].replace(headline, \"\",True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
