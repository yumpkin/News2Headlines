{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "brief-liability",
   "metadata": {},
   "source": [
    "## Analysis of headlines #1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-sandwich",
   "metadata": {},
   "source": [
    "In this task we are going to use the Paraphraser dataset to do basic text analysis operations on news headlines.\n",
    "The dataset is a large .json file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-girlfriend",
   "metadata": {},
   "source": [
    "### Getting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "million-tanzania",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import remove\n",
    "from shutil import copyfileobj\n",
    "import zipfile\n",
    "import requests\n",
    "\n",
    "def download_file(url):\n",
    "    compressed_dataset = 'Headlines.zip'\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        with open(compressed_dataset, 'wb') as f:\n",
    "            copyfileobj(r.raw, f)\n",
    "            \n",
    "    archive = zipfile.ZipFile(compressed_dataset)\n",
    "    for file in archive.namelist():\n",
    "        if file.endswith('.json'):\n",
    "            archive.extract(file)\n",
    "            archive.close() # close file\n",
    "            os.remove(compressed_dataset) \n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "promising-vehicle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_file('http://paraphraser.ru/download/get?file_id=7')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-ethics",
   "metadata": {},
   "source": [
    "### Getting familiar with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-register",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"ParaPhraserPlus/ParaPhraserPlus.json\") as data_set:\n",
    "    HEADLINE_SET = json.load(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-treasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADLINE_SET['0'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-camcorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(HEADLINE_SET.items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-citizen",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(HEADLINE_SET.items())[0][1]['headlines']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-wales",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(HEADLINE_SET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-great",
   "metadata": {},
   "source": [
    "### Starting the stage of analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-graham",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk, razdel\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Download nltk packages used in this example\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-vacation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize list of stopwords as needed. Here, we add common\n",
    "# punctuation and contraction artifacts.\n",
    "punctuations = list(string.punctuation)\n",
    "stop_words = stopwords.words('russian') + punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-shark",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_preprocess_headlines(article_id):\n",
    "    sentences = []\n",
    "    tokens = []\n",
    "    \n",
    "    rubric = list(HEADLINE_SET.items())[article_id][1]['rubric']\n",
    "    headlines = list(HEADLINE_SET.items())[article_id][1]['headlines']\n",
    "    \n",
    "    sentences += [_.text for each_headline in headlines for _ in razdel.sentenize(each_headline)]\n",
    "    tokens += [token.text.lower() for sentence in sentences \n",
    "               for token in razdel.tokenize(sentence)]\n",
    "        \n",
    "    corpus = [word for word in tokens if word[0] not in stop_words]\n",
    "    fdist = nltk.FreqDist(corpus)\n",
    "    return (article_id, rubric, sentences, tokens, corpus, fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-wellington",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_id, rubric, sentences, tokens, corpus, fdist = basic_preprocess_headlines(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-stack",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "from IPython.display import display\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "mystem = Mystem()\n",
    "#Preprocess function\n",
    "def informative_preprocess_headline(sentence):\n",
    "    lemmas = mystem.lemmatize(sentence.lower())\n",
    "    sentence_info = mystem.analyze(sentence)\n",
    "    analysis_info = list(filter(lambda x: 'analysis' in x, sentence_info))\n",
    "    \n",
    "    lemmas = [lemma for lemma in lemmas if lemma not in stop_words\\\n",
    "              and lemma != \" \" \\\n",
    "              and lemma.strip() not in punctuations]\n",
    "\n",
    "    sentence = \" \".join(lemmas)\n",
    "    processed_sentence = [word for word in sentence.split(' ')]\n",
    "\n",
    "    return (processed_sentence, analysis_info)\n",
    "\n",
    "    # Basic stats\n",
    "def headline_stats():\n",
    "    num_words = sum([i for i in fdist.values()])\n",
    "    num_unique_words = len(fdist.items())\n",
    "\n",
    "    average_word_length = round(sum(len(word) for word in corpus) / len(corpus), 2)\n",
    "    max_word_length = np.max(np.array([len(word) for word in corpus]))\n",
    "    average_sentence_length = round(len(corpus) / len(sentences), 2)\n",
    "\n",
    "    # Hapaxes are words that appear only once\n",
    "    num_hapaxes = len(fdist.hapaxes())\n",
    "    top_n_words = fdist.most_common(10)\n",
    "        \n",
    "    print(\"---------------------Article {%d}-----------------------\" % article_id)\n",
    "    print(rubric)\n",
    "    print('\\tMaximum word length in headline:'.ljust(25), max_word_length)\n",
    "    print('\\tAverage length of a word (letterwise):'.ljust(25), average_word_length)\n",
    "    print('\\tAverage length headline length(wordwise):'.ljust(25), average_sentence_length)\n",
    "    print('\\tNum Sentences:'.ljust(25), len(sentences))\n",
    "    print('\\tNum Words:'.ljust(25), num_words)\n",
    "    print('\\tNum Unique Words:'.ljust(25), num_unique_words)\n",
    "    print('\\tNum Hapaxes:'.ljust(25), num_hapaxes)\n",
    "    print('\\tTop 10 Most Frequent Words (stop words):\\n\\t\\t', \\\n",
    "            '\\n\\t\\t'.join(['%s (%s)'\n",
    "            % (w[0], w[1]) for w in top_n_words]))\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    data = []\n",
    "\n",
    "    for word,freq in fdist.most_common(10):\n",
    "        data.append({'Word':word,'Frequency':freq})\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Create a bar graph with the most commonly used descriptors\n",
    "    df['Word']=df['Word'].str.capitalize()\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "    df.plot(kind='barh', x='Word',y='Frequency', ax=axes[0], figsize=(15,5),color='#cb101c')\n",
    "    df.plot(kind='line', x='Word',y='Frequency', ax=axes[1], figsize=(15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impaired-diversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _score_sentences(sentences, important_words):\n",
    "    N = 3  # Number of words to consider\n",
    "    CLUSTER_DIST = 3  # Distance between words to consider\n",
    "    scores = []\n",
    "    sentence_idx = -1\n",
    "\n",
    "    for s in [nltk.tokenize.word_tokenize(s) for s in sentences]:\n",
    "\n",
    "        sentence_idx += 1\n",
    "        word_idx = []\n",
    "\n",
    "        # For each word in the word list...\n",
    "        for w in important_words:\n",
    "            try:\n",
    "                # Compute an index for where any important words occur in the sentence.\n",
    "\n",
    "                word_idx.append(s.index(w[0]))\n",
    "            except ValueError: # w not in this particular sentence\n",
    "                pass\n",
    "\n",
    "        word_idx.sort()\n",
    "\n",
    "        # It is possible that some sentences may not contain any important words at all.\n",
    "        if len(word_idx)== 0: continue\n",
    "\n",
    "        # Using the word index, compute clusters by using a max distance threshold\n",
    "        # for any two consecutive words.\n",
    "\n",
    "        clusters = []\n",
    "        cluster = [word_idx[0]]\n",
    "        i = 1\n",
    "        while i < len(word_idx):\n",
    "            if word_idx[i] - word_idx[i - 1] < CLUSTER_DIST:\n",
    "                cluster.append(word_idx[i])\n",
    "            else:\n",
    "                clusters.append(cluster[:])\n",
    "                cluster = [word_idx[i]]\n",
    "            i += 1\n",
    "        clusters.append(cluster)\n",
    "\n",
    "        # Score each cluster. The max score for any given cluster is the score \n",
    "        # for the sentence.\n",
    "\n",
    "        max_cluster_score = 0\n",
    "        for c in clusters:\n",
    "            significant_words_in_cluster = len(c)\n",
    "            total_words_in_cluster = c[-1] - c[0] + 1\n",
    "            score = 1.0 * significant_words_in_cluster \\\n",
    "                * significant_words_in_cluster / total_words_in_cluster\n",
    "\n",
    "            if score > max_cluster_score:\n",
    "                max_cluster_score = score\n",
    "\n",
    "        scores.append((sentence_idx, score))\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-relative",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_top_headlines_by_score(NUM_OF_TOP_WORDS):\n",
    "    \n",
    "    top_n_words = fdist.most_common(NUM_OF_TOP_WORDS)\n",
    "    normalized_sentences = [s.lower() for s in sentences]\n",
    "    NUM_TOP_SCORED_HEADLINES = 5  # Number of sentences to return for a \"top n\" summary\n",
    "    scored_sentences = _score_sentences(normalized_sentences, top_n_words)\n",
    "\n",
    "    # Summarization Approach 1:\n",
    "    # Filter out nonsignificant sentences by using the average score plus a\n",
    "    # fraction of the std dev as a filter\n",
    "\n",
    "    avg = np.mean([s[1] for s in scored_sentences])\n",
    "    std = np.std([s[1] for s in scored_sentences])\n",
    "    mean_scored = [(sent_idx, score) for (sent_idx, score) in scored_sentences\n",
    "                   if score > avg + 0.5 * std]\n",
    "\n",
    "    # Summarization Approach 2:\n",
    "    # Another approach would be to return only the top N ranked sentences\n",
    "\n",
    "    top_n_scored = sorted(scored_sentences, key=lambda s: s[1])[-NUM_TOP_SCORED_HEADLINES:]\n",
    "    top_n_scored = sorted(top_n_scored, key=lambda s: s[0])\n",
    "    \n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(\"-------HEADLINES(Picked/blue -- original/black)--------\")\n",
    "    top_n_summary=[sentences[idx] for (idx, score) in top_n_scored]\n",
    "    mean_scored_summary=[sentences[idx] for (idx, score) in mean_scored]\n",
    "    for sen in sentences:\n",
    "        if (sen) in set(mean_scored_summary and top_n_summary):\n",
    "            print(\"\\x1b[94m\"+sen+\"\\x1b[0m\"+\"\\t\")\n",
    "        else:\n",
    "            print(sen)\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    for (idx, score) in mean_scored:\n",
    "        print(sentences[idx], score)\n",
    "    print(\"------------------------------------------------------\")\n",
    "    print(\"-------------------Lemmatized text--------------------\")\n",
    "    print(\"------------------------------------------------------\")\n",
    "    for top_sentence in top_n_summary:\n",
    "        lemmatized_sentence, analysis_info = informative_preprocess_headline(top_sentence)\n",
    "        print(\"Original =>\\t\"+top_sentence)\n",
    "        print(\"Preprocessed =>\\t\"+\"\\x1b[35m\" +' '.join(lemmatized_sentence) +\"\\x1b[0m\")\n",
    "        for i in analysis_info:\n",
    "            print(\"word info:\", i['analysis'], i['text'])\n",
    "        print(\"=================================================\")\n",
    "    print(\"-----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-production",
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_stats() # article_id being passed as parameter, data aggregation is also a possible option for later.\n",
    "# print the most suitable headlines ranked by score, based on the simple scoring approaches we have used.\n",
    "pick_top_headlines_by_score(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leading-rates",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-reader",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "massive-superintendent",
   "metadata": {},
   "source": [
    "#### Comment: \n",
    "From the resul.ts that we have obtained, we can see that the way we scored the sentences is good, but can be improved a bit.. for instance, the sentence with the highest score, has the word \"Messanger\", which is redundant, also it does not give any information like when this will happen and is it for all phones or only for old, etc. This happens because the score mechanism that we have used has no information about the entities that each word represnts, having information about the type/entities that a word represnts, can help to have a better scoring mechanism if we give entites like \"Date\" more importance, while giving words like \"Messanger\" less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lined-antenna",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_entity():\n",
    "        pos_tagged_tokens = [nltk.pos_tag(t) for t in tokens]\n",
    "\n",
    "    # Flatten the list since we're not using sentence structure\n",
    "    # and sentences are guaranteed to be separated by a special\n",
    "    # POS tuple such as ('.', '.')\n",
    "\n",
    "    pos_tagged_tokens = [token for sent in pos_tagged_tokens for token in sent]\n",
    "\n",
    "    all_entity_chunks = []\n",
    "    previous_pos = None\n",
    "    current_entity_chunk = []\n",
    "    for (token, pos) in pos_tagged_tokens:\n",
    "\n",
    "        if pos == previous_pos and pos.startswith('NN'):\n",
    "            current_entity_chunk.append(token)\n",
    "        elif pos.startswith('NN'):\n",
    "            if current_entity_chunk != []:\n",
    "\n",
    "                # Note that current_entity_chunk could be a duplicate when appended,\n",
    "                # so frequency analysis again becomes a consideration\n",
    "\n",
    "                all_entity_chunks.append((' '.join(current_entity_chunk), pos))\n",
    "            current_entity_chunk = [token]\n",
    "\n",
    "        previous_pos = pos\n",
    "\n",
    "    # Store the chunks as an index for the document\n",
    "    # and account for frequency while we're at it...\n",
    "\n",
    "#     article_headlines = {}\n",
    "#     for c in all_entity_chunks:\n",
    "#         article_headlines['entities'][c] = article_headlines['entities'].get(c, 0) + 1\n",
    "\n",
    "#     # For example, we could display just the title-cased entities\n",
    "\n",
    "#     proper_nouns = []\n",
    "#     for (entity, pos) in article_headlines['entities']:\n",
    "#         if entity.istitle():\n",
    "#             print('\\t%s (%s)' % (entity, article_headlines['entities'][(entity, pos)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encouraging-andorra",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipymarkup import show_span_box_markup, show_dep_markup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owned-twenty",
   "metadata": {},
   "source": [
    "### Bag Of Words for the headlines of each article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-particular",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-istanbul",
   "metadata": {},
   "source": [
    "### n-grams model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-video",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "level-musician",
   "metadata": {},
   "source": [
    "### LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-details",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-shannon",
   "metadata": {},
   "source": [
    "#### Intertopic Distance Plot can help you learn about how topics relate to each other, including potential higher-level structure between groups of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-scheduling",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-checklist",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
