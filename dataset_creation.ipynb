{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "sealed-mustang",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "import csv\n",
    "\n",
    "import json\n",
    "import hashlib\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "pressed-nickname",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "chmod +x chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "racial-conclusion",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aliak/.local/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "binary_yandex_driver_file = './chromedriver'\n",
    "driver = webdriver.Chrome(binary_yandex_driver_file, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "driving-honolulu",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_to_csv(category, url):\n",
    "    driver.get(url)\n",
    "    time.sleep(2)  # Allow 2 seconds for the web page to open\n",
    "    scroll_pause_time = 1 # You can set your own pause time. My laptop is a bit slow so I use 1 sec\n",
    "    screen_height = driver.execute_script(\"return window.screen.height;\")   # get the screen height of the web\n",
    "    i = 1\n",
    "\n",
    "    while True:\n",
    "        # scroll one screen height each time\n",
    "        driver.execute_script(\"window.scrollTo(0, {screen_height}*{i});\".format(screen_height=screen_height, i=i))  \n",
    "        i += 1\n",
    "        time.sleep(scroll_pause_time)\n",
    "        # update scroll height each time after scrolled, as the scroll height can change after we scrolled the page\n",
    "        scroll_height = driver.execute_script(\"return document.body.scrollHeight;\")  \n",
    "        # Break the loop when the height we need to scroll to is larger than the total scroll height\n",
    "        if (screen_height) * i > scroll_height:\n",
    "            break \n",
    "        articles = {}\n",
    "        \n",
    "    directory_to_save = './rbc_links_parser/'\n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir(directory_to_save)\n",
    "    except FileExistsError:\n",
    "        print(\"Directory \" , directory_to_save ,  \" already exists\")\n",
    "        \n",
    "    with open(directory_to_save+'rbc_'+category+'.csv', 'w') as f:\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        articles = {}\n",
    "        for element in soup.body.findAll('div', attrs={'class': 'item item_image-mob js-category-item'}):\n",
    "            articles['date'] = element.select_one('span.item__category').text.strip()\n",
    "            articles['title'] = element.select_one('span.rm-cm-item-text').text.strip()\n",
    "            articles['url'] = element.find('a').get('href')\n",
    "            w = csv.DictWriter(f, articles.keys())\n",
    "            w.writerow(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "spiritual-screening",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  ./rbc_links_parser/  already exists\n",
      "Directory  ./rbc_links_parser/  already exists\n",
      "Directory  ./rbc_links_parser/  already exists\n",
      "Directory  ./rbc_links_parser/  already exists\n",
      "Directory  ./rbc_links_parser/  already exists\n"
     ]
    }
   ],
   "source": [
    "link_to_parse = {}\n",
    "link_to_parse['politics'] = 'https://www.rbc.ru/politics/?utm_source=topline'\n",
    "link_to_parse['economics'] = 'https://www.rbc.ru/economics/?utm_source=topline'\n",
    "link_to_parse['society'] = 'https://www.rbc.ru/society/?utm_source=topline'\n",
    "link_to_parse['business'] = 'https://www.rbc.ru/business/?utm_source=topline'\n",
    "link_to_parse['tech'] = 'https://www.rbc.ru/technology_and_media/?utm_source=topline'\n",
    "link_to_parse['finance'] = 'https://www.rbc.ru/finances/?utm_source=topline'\n",
    "\n",
    "for category, link in link_to_parse.items():\n",
    "    parse_to_csv(category, link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "warming-tennessee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "\n",
    "def parse(link):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "    session = requests.Session()\n",
    "    retry = Retry(connect=3, backoff_factor=0.5)\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    session.get(link, headers=headers)\n",
    "    response = session.get(link, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "innocent-distance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_entry(dict_entry):\n",
    "    try:\n",
    "        return {hashlib.md5(dict_entry['body'].encode()).hexdigest(): dict_entry}\n",
    "    except Exception as e:\n",
    "        print(e, dict_entry)\n",
    "        \n",
    "for file_name in glob.glob('./rbc_links_parser/'+'*.csv'):\n",
    "    df = pd.read_csv(file_name, names=['url', 'title', 'date'])\n",
    "    links= df.url\n",
    "    for link in links:\n",
    "        soup = parse(link)\n",
    "        corpus = {}\n",
    "        trash = ['article__main-image', 'article__header__info-block', 'pro-anons', 'article__authors', 'social-networks__content', 'article__inline-item']\n",
    "        \n",
    "        for text_div in soup.findAll('div', attrs={'class':'l-col-main'}):\n",
    "            extracted_trash = [t.extract() for trash_class in trash for t in text_div.findAll('div', attrs={'class':trash_class})]\n",
    "            del extracted_trash\n",
    "\n",
    "            for tag_bar in text_div.findAll('div', attrs={'class':'article__tags__container'}):\n",
    "                tags = [tag.string for tag in tag_bar.findAll('a') if tag_bar is not None ]\n",
    "                \n",
    "            headline = text_div.find('h1')\n",
    "            overview = text_div.find('div', attrs = {'article__text__overview'})\n",
    "            if text_div is not None and overview is not None and headline is not None and tags is not None:\n",
    "                corpus['headline'] = str(headline.text).strip()\n",
    "                corpus['overview'] = overview.span.text\n",
    "                corpus['category'] = str(file_name.split('_')[3].split('.')[0])\n",
    "                corpus['tags'] = tags\n",
    "                tag_bar.extract()\n",
    "                corpus['body'] = (re.sub(r'\\s+', ' ', text_div.text))\n",
    "            else:\n",
    "                continue\n",
    "        if bool(corpus):        \n",
    "            entry = add_entry(corpus)\n",
    "        try :\n",
    "            with open('data_file.json') as json_file:\n",
    "                data = json.load(json_file)\n",
    "        except :\n",
    "            data = {}\n",
    "            \n",
    "        data.update(entry)\n",
    "\n",
    "        with open(\"data_file.json\", \"w+\") as write_file:\n",
    "            json.dump(data, write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "danish-withdrawal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bigjson\n",
    "def query_big_json(article_id):\n",
    "    with open('./data_file.json', 'rb') as data_set:\n",
    "        data = bigjson.load(data_set)\n",
    "        corpus_data = data[str(article_id)]\n",
    "        category = corpus_data['category']\n",
    "        text = corpus_data['body']\n",
    "        \n",
    "    return (article_id, category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "designing-softball",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('30353924877bc034ba054e46311f301b', 'business')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's randomly pick a hash and test it\n",
    "import random\n",
    "df = pd.read_json('data_file.json')\n",
    "df = df.transpose()\n",
    "h = random.choice(list(df.index))\n",
    "\n",
    "query_big_json('30353924877bc034ba054e46311f301b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "surgical-remainder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags</th>\n",
       "      <th>overview</th>\n",
       "      <th>headline</th>\n",
       "      <th>body</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30353924877bc034ba054e46311f301b</th>\n",
       "      <td>[вакцинация от COVID-19, ВТО, экспорт, базы да...</td>\n",
       "      <td>Россия занимает пятое место в мире по объему п...</td>\n",
       "      <td>Россия заняла пятое место в мире по экспорту в...</td>\n",
       "      <td>Россия заняла пятое место в мире по экспорту ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fc0b7fb1406eb3bb4f29a9335e25d65e</th>\n",
       "      <td>[вакцинация от COVID-19, ВТО, экспорт, базы да...</td>\n",
       "      <td>Россия занимает пятое место в мире по объему п...</td>\n",
       "      <td>Россия заняла пятое место в мире по экспорту в...</td>\n",
       "      <td>Россия заняла пятое место в мире по экспорту ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eb08734fd3d421e15ebbcc81f53398b9</th>\n",
       "      <td>[черная металлургия, водородная энергетика]</td>\n",
       "      <td>Владелец «Северстали» Алексей Мордашов оценил ...</td>\n",
       "      <td>Мордашов оценил переход российской металлургии...</td>\n",
       "      <td>Мордашов оценил переход российской металлурги...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                               tags  \\\n",
       "30353924877bc034ba054e46311f301b  [вакцинация от COVID-19, ВТО, экспорт, базы да...   \n",
       "fc0b7fb1406eb3bb4f29a9335e25d65e  [вакцинация от COVID-19, ВТО, экспорт, базы да...   \n",
       "eb08734fd3d421e15ebbcc81f53398b9        [черная металлургия, водородная энергетика]   \n",
       "\n",
       "                                                                           overview  \\\n",
       "30353924877bc034ba054e46311f301b  Россия занимает пятое место в мире по объему п...   \n",
       "fc0b7fb1406eb3bb4f29a9335e25d65e  Россия занимает пятое место в мире по объему п...   \n",
       "eb08734fd3d421e15ebbcc81f53398b9  Владелец «Северстали» Алексей Мордашов оценил ...   \n",
       "\n",
       "                                                                           headline  \\\n",
       "30353924877bc034ba054e46311f301b  Россия заняла пятое место в мире по экспорту в...   \n",
       "fc0b7fb1406eb3bb4f29a9335e25d65e  Россия заняла пятое место в мире по экспорту в...   \n",
       "eb08734fd3d421e15ebbcc81f53398b9  Мордашов оценил переход российской металлургии...   \n",
       "\n",
       "                                                                               body  \\\n",
       "30353924877bc034ba054e46311f301b   Россия заняла пятое место в мире по экспорту ...   \n",
       "fc0b7fb1406eb3bb4f29a9335e25d65e   Россия заняла пятое место в мире по экспорту ...   \n",
       "eb08734fd3d421e15ebbcc81f53398b9   Мордашов оценил переход российской металлурги...   \n",
       "\n",
       "                                  category  \n",
       "30353924877bc034ba054e46311f301b  business  \n",
       "fc0b7fb1406eb3bb4f29a9335e25d65e  business  \n",
       "eb08734fd3d421e15ebbcc81f53398b9  business  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "exterior-morning",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def remove_article(article_id):\n",
    "    with open('data_file.json', 'r') as data_file:\n",
    "        data = json.load(data_file)\n",
    "        data.pop(article_id, None)\n",
    "\n",
    "    with open('data_file.json', 'w') as data_file:\n",
    "        data = json.dump(data, data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "known-dream",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "def hash_it(file):\n",
    "    hashed_articles = dict()\n",
    "\n",
    "    category = str(file.split('_')[1].split('.')[0])\n",
    "    BLOCK_SIZE = 65536 # The size of each read from the file\n",
    "\n",
    "    file_hash = hashlib.sha256() # Create the hash object, can use something other than `.sha256()` if you wish\n",
    "    with open(file, 'rb') as f: # Open the file to read it's bytes\n",
    "        fb = f.read(BLOCK_SIZE) # Read from the file. Take in the amount declared above\n",
    "        while len(fb) > 0: # While there is still data being read from the file\n",
    "            file_hash.update(fb) # Update the hash\n",
    "            fb = f.read(BLOCK_SIZE) # Read the next block from the file\n",
    "\n",
    "    hashed_articles[file_hash.hexdigest()] = category\n",
    "    return hashed_articles # Get the hexadecimal digest of the hash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appointed-commission",
   "metadata": {},
   "source": [
    "It would be useful to have two dictionaries one that have the hashed text body of the articles as its key\\\n",
    "(this dictionary is the one with labeled category)\\\n",
    "And the other one is the one that we will make after we do topic modelling\\\n",
    "Then we want to get an insight about our articles we just compare the hashes, and then understand the pattern behind the clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "piano-beatles",
   "metadata": {},
   "outputs": [],
   "source": [
    "as_original = hash_it(\"oo_politics.txt\")\n",
    "as_original.update(hash_it(\"rbc_politics.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "impressive-stick",
   "metadata": {},
   "outputs": [],
   "source": [
    "as_classified = {'e6ac6518a2afbe64f54b5b590a79c25c7acd6978bdb1b4fcd8782bb928e2ba96': 'economy',\n",
    "     'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855': 'politics'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "enhanced-drink",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'e6ac6518a2afbe64f54b5b590a79c25c7acd6978bdb1b4fcd8782bb928e2ba96': 'economy',\n",
       " 'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855': 'politics'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "as_classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "unsigned-passenger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'e6ac6518a2afbe64f54b5b590a79c25c7acd6978bdb1b4fcd8782bb928e2ba96': 'economy'}\n"
     ]
    }
   ],
   "source": [
    "misclassified = {k: as_classified[k] for k in as_original if k in as_classified and as_original[k] != as_classified[k]}\n",
    "print(misclassified)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
