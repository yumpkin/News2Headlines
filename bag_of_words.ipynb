{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rapid-configuration",
   "metadata": {},
   "source": [
    "```\n",
    "Let's build our bag of words to be used for topic modelling\n",
    "Accordig to bigARTM a dataset like UCI bag-of-words has a suitable format,\n",
    "so we would make a bag-of-word vectors out of our own dataset in the same format as the UCI one.\n",
    "\n",
    "For each text collection, D is the number of documents, W is the\n",
    "number of words in the vocabulary, and N is the total number of words\n",
    "in the collection (below, NNZ is the number of nonzero counts in the\n",
    "bag-of-words). After tokenization and removal of stopwords, the\n",
    "vocabulary of unique words was truncated by only keeping words that\n",
    "occurred more than ten times.\n",
    "\n",
    "These data sets are ideal\n",
    "for clustering and topic modeling experiments.\n",
    "\n",
    "For each text collection we provide docword.*.txt (the bag of words\n",
    "file in sparse format) and vocab.*.txt (the vocab file).\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "secret-binary",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/aliak/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/aliak/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import nltk, razdel\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "# Download nltk packages used in this example\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "federal-retro",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bigjson\n",
    "def query_big_json(article_id):\n",
    "    with open('data_file.json', 'rb') as data_set:\n",
    "        data = bigjson.load(data_set)\n",
    "        corpus_data = data[str(article_id)]\n",
    "        category = corpus_data['category']\n",
    "        text = corpus_data['body']\n",
    "        \n",
    "    return (article_id, category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "central-parking",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('data_file.json')\n",
    "df = df.transpose()\n",
    "df = df[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "extended-valuation",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = [text for text in df['body'][0:10] ]\n",
    "random.shuffle(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "silver-lodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = list(string.punctuation)\n",
    "stop_words_ru = stopwords.words('russian') + punctuations\n",
    "\n",
    "def preprocess_text(document):\n",
    "    articles = []\n",
    "    for document in document:\n",
    "\n",
    "        stemmer = Mystem()\n",
    "        # Remove all the special characters\n",
    "        document = re.sub(r'\\W', ' ', str(document))\n",
    "\n",
    "        # remove all single characters\n",
    "        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "        # Remove single characters from the start\n",
    "        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "        # Substituting multiple spaces with single space\n",
    "        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "\n",
    "        # Converting to Lowercase\n",
    "        document = document.lower()\n",
    "\n",
    "        # Lemmatization\n",
    "        sentences = document.split()\n",
    "        tokens = [token.text for sentence in sentences \n",
    "               for token in razdel.tokenize(sentence)]\n",
    "        tokens = [stemmer.lemmatize(word)[0] for word in tokens]\n",
    "        tokens = [word for word in tokens if not word.isdigit() and word not in stop_words_ru]\n",
    "        tokens = [word for word in tokens if len(word)  > 3]\n",
    "        text = \" \".join(tokens)\n",
    "        articles.append(text)\n",
    "    \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "synthetic-biology",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 1000\n",
    "n_components = len(set(df.category))\n",
    "n_top_words = 20 #number of top words to be extracted for a single topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "internal-service",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def count_vectorize(articles):\n",
    "    \n",
    "    corpus = preprocess_text(articles)\n",
    "    \n",
    "    vectorizer = CountVectorizer(\n",
    "        max_df=0.95,\n",
    "        min_df=2, \n",
    "        max_features=n_features,\n",
    "        stop_words=stop_words_ru,\n",
    "        ngram_range=(1, 1))\n",
    "\n",
    "    token_count_matrix = vectorizer.fit_transform(corpus)\n",
    "    features = vectorizer.get_feature_names()\n",
    "        \n",
    "    vocab = open(\"vocab.rbc.txt\", \"w\")\n",
    "    for feature in features:\n",
    "        vocab.write(feature+'\\n')\n",
    "    vocab.close()\n",
    "    \n",
    "    docword = open(\"docword.rbc.txt\", \"w\")\n",
    "    cx = token_count_matrix.tocoo()\n",
    "    for docID, wordID, wordCount in zip(cx.row, cx.col, cx.data):\n",
    "        wordID += 1 # making it unity based to suit with bigARTM\n",
    "        docID += 1\n",
    "        docword.write(f\"{docID} {wordID} {wordCount}\\n\")\n",
    "    docword.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "realistic-assumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorize(articles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
